from datasets import Dataset
from ragas import evaluate
from ragas.metrics import answer_relevancy, context_precision, faithfulness

# 1. Giáº£ láº­p hoáº·c load táº­p dá»¯ liá»‡u kiá»ƒm thá»­ (Ground Truth)
test_data = {
    "question": ["LÃ m tháº¿ nÃ o Ä‘á»ƒ khá»Ÿi táº¡o LangGraph?", "Hybrid Search lÃ  gÃ¬?"],
    "contexts": [
        ["DÃ¹ng StateGraph tá»« langgraph.graph..."],
        ["Káº¿t há»£p Vector vÃ  BM25..."],
    ],
    "answer": ["Báº¡n dÃ¹ng StateGraph.", "LÃ  sá»± káº¿t há»£p tÃ¬m kiáº¿m dÃ y Ä‘áº·c vÃ  thÆ°a thá»›t."],
    "ground_truth": [
        "Äá»ƒ khá»Ÿi táº¡o LangGraph, cáº§n Ä‘á»‹nh nghÄ©a StateGraph vÃ  Nodes.",
        "Hybrid Search lÃ  ká»¹ thuáº­t káº¿t há»£p giá»¯a Vector Search vÃ  BM25.",
    ],
}


def run_assessment():
    dataset = Dataset.from_dict(test_data)

    # 2. Cháº¡y Ä‘Ã¡nh giÃ¡
    result = evaluate(
        dataset,
        metrics=[faithfulness, answer_relevancy, context_precision],
    )

    # 3. Xuáº¥t káº¿t quáº£ ra Ä‘á»‹nh dáº¡ng Markdown cho GitHub Action
    df = result.to_pandas()
    with open("report.md", "w") as f:
        f.write("### ğŸ“Š RAG Evaluation Report\n")
        f.write(df.to_markdown())


if __name__ == "__main__":
    run_assessment()
